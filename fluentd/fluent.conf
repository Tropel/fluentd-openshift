# This configuration file for Fluentd / td-agent is used
# to watch changes to Docker log files. The kubelet creates symlinks that
# capture the pod name, namespace, container name & Docker container ID
# to the docker logs for pods in /var/log/containers directory on the host.
# If running this fluentd configuration in a Docker container, the /var/log
# directory should be mounted in the container.
#
# These logs are then submitted to # Elasticsearch which assumes the
# installation of the fluent-plugin-elasticsearch & the
# fluent-plugin-kubernetes_metadata_filter plugins.
# See https://github.com/uken/fluent-plugin-elasticsearch &
# https://github.com/fabric8io/fluent-plugin-kubernetes_metadata_filter for
# more information about the plugins.
# Maintainer: Jimmi Dyson <jimmidyson@gmail.com>
#
# Example
# =======
# A line in the Docker log file might look like this JSON:
#
# {"log":"2014/09/25 21:15:03 Got request with path wombat\n",
#  "stream":"stderr",
#   "time":"2014-09-25T21:15:03.499185026Z"}
#
# The time_format specification below makes sure we properly
# parse the time format produced by Docker. This will be
# submitted to Elasticsearch and should appear like:
# $ curl 'http://elasticsearch-logging.default:9200/_search?pretty'
# ...
# {
#      "_index" : "logstash-2014.09.25",
#      "_type" : "fluentd",
#      "_id" : "VBrbor2QTuGpsQyTCdfzqA",
#      "_score" : 1.0,
#      "_source":{"log":"2014/09/25 22:45:50 Got request with path wombat\n",
#                 "stream":"stderr","tag":"docker.container.all",
#                 "@timestamp":"2014-09-25T22:45:50+00:00"}
#    },
# ...
#
# The Kubernetes fluentd plugin is used to write the Kubernetes metadata to the log
# record & add labels to the log record if properly configured. This enables users
# to filter & search logs on any metadata.
# For example a Docker container's logs might be in the directory:
#
#  /var/lib/docker/containers/997599971ee6366d4a5920d25b79286ad45ff37a74494f262e3bc98d909d0a7b
#
# and in the file:
#
#  997599971ee6366d4a5920d25b79286ad45ff37a74494f262e3bc98d909d0a7b-json.log
#
# where 997599971ee6... is the Docker ID of the running container.
# The Kubernetes kubelet makes a symbolic link to this file on the host machine
# in the /var/log/containers directory which includes the pod name and the Kubernetes
# container name:
#
#    synthetic-logger-0.25lps-pod_default_synth-lgr-997599971ee6366d4a5920d25b79286ad45ff37a74494f262e3bc98d909d0a7b.log
#    ->
#    /var/lib/docker/containers/997599971ee6366d4a5920d25b79286ad45ff37a74494f262e3bc98d909d0a7b/997599971ee6366d4a5920d25b79286ad45ff37a74494f262e3bc98d909d0a7b-json.log
#
# The /var/log directory on the host is mapped to the /var/log directory in the container
# running this instance of Fluentd and we end up collecting the file:
#
#   /var/log/containers/synthetic-logger-0.25lps-pod_default_synth-lgr-997599971ee6366d4a5920d25b79286ad45ff37a74494f262e3bc98d909d0a7b.log
#
# This results in the tag:
#
#  var.log.containers.synthetic-logger-0.25lps-pod_default_synth-lgr-997599971ee6366d4a5920d25b79286ad45ff37a74494f262e3bc98d909d0a7b.log
#
# The Kubernetes fluentd plugin is used to extract the namespace, pod name & container name
# which are added to the log message as a kubernetes field object & the Docker container ID
# is also added under the docker field object.
# The final tag is:
#
#   kubernetes.var.log.containers.synthetic-logger-0.25lps-pod_default_synth-lgr-997599971ee6366d4a5920d25b79286ad45ff37a74494f262e3bc98d909d0a7b.log
#
# And the final log record look like:
#
# {
#   "log":"2014/09/25 21:15:03 Got request with path wombat\n",
#   "stream":"stderr",
#   "time":"2014-09-25T21:15:03.499185026Z",
#   "kubernetes": {
#     "namespace": "default",
#     "pod_name": "synthetic-logger-0.25lps-pod",
#     container_name: "synth-lgr"
#   },
#   "docker": {
#     "container_id": "997599971ee6366d4a5920d25b79286ad45ff37a74494f262e3bc98d909d0a7b"
#   }
# }
#
# This makes it easier for users to search for logs by pod name or by
# the name of the Kubernetes container regardless of how many times the
# Kubernetes pod has been restarted (resulting in a several Docker container IDs).

<system>
  log_level warn
</system>

#
#  This <source> looks to match messages from /var/log/messages*.
#
#  The following are examples of what should be matched:
#  1. Sep 10 22:19:28 localhost kernel: Blkfront and the Xen platform PCI driver have been compiled for this kernel: unplug emulated disks.
#     You might have to change the root device
#     from /dev/hd[a-d] to /dev/xvd[a-d]
#     in your root= kernel command line option
#
#  2. Nov 6 11:24:03 dhcp-32-77 dbus[1236]: [system] Activating via systemd: service name='org.freedesktop.hostname1' unit='dbus-org.freedesktop.hostname1.service'
#
#  3. Aug 17 08:11:18 tcp9-54-255-137-97.example.com NetworkManager[1257]: <info>  VPN service 'libreswan' started (org.freedesktop.NetworkManager.libreswan), PID 27699
#
#
#  The following are examples of what should NOT be matched:
#  1. December 24 08:11:35 dhcp-22-55 fprintd: ** Message: No devices in use, exit
#
#  2. Jan 22  6:1:2 dhcp-36-79 kernel: IPv4 over IPSec tunneling driver
#
#  3. Apr8 15:20:01 dhcp-42-42 systemd: Created slice user-0.slice.
#
<source>
  @type tail
  @label @SYSTEM
  path /var/log/messages*
  pos_file /var/log/node.log.pos
  tag system.*
  format multiline
  # Begin possible multiline match: "Mmm DD HH:MM:SS "
  format_firstline /^[A-Z][a-z]{2}\s+[0-3]?[0-9]\s+[0-2][0-9]:[0-5][0-9]:[0-6][0-9]\s/
  # extract metadata from same line that matched format_firstline
  format1 /^(?<time>\S+\s+\S+\s+\S+)\s+(?<host>\S+)\s+(?<ident>[\w\/\.\-]*)(?:\[(?<pid>[0-9]+)\])?[^\:]*\:\s*(?<message>.*)$/
  time_format %b %d %H:%M:%S
  read_from_head true
  keep_time_key true
</source>

<source>
  @type tail
  @label @KUBERNETES
  path /var/log/containers/*.log
  pos_file /var/log/es-containers.log.pos
  time_format %Y-%m-%dT%H:%M:%S
  tag kubernetes.*
  format json
  keep_time_key true
  read_from_head true
</source>

<label @KUBERNETES>
  <filter kubernetes.**>
    type kubernetes_metadata
    kubernetes_url "#{ENV['K8S_HOST_URL']}"
    bearer_token_file /var/run/secrets/kubernetes.io/serviceaccount/token
    ca_file /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
  </filter>
  <filter **kibana**>
    type record_transformer
    enable_ruby
    <record>
      log ${(err || msg) || log}
    </record>
    remove_keys req,res,msg,name,level,v,pid,err
  </filter>
  <filter kubernetes.**>
    type flatten_hash
    separator _
  </filter>
  <filter kubernetes.**>
    type record_transformer
    enable_ruby
    <record>
      hostname ${kubernetes_host}
      message ${log}
      version 1.0.6
    </record>
    remove_keys log,stream
  </filter>

  <match **_default_** **_openshift_** **_openshift-infra_**>
    @type elasticsearch_dynamic
    host "#{ENV['OPS_HOST']}"
    port "#{ENV['OPS_PORT']}"
    scheme https
    index_name .operations.${Time.at(time).getutc.strftime(@logstash_dateformat)}

    user fluentd
    password changeme

    client_key "#{ENV['OPS_CLIENT_KEY']}"
    client_cert "#{ENV['OPS_CLIENT_CERT']}"
    ca_file "#{ENV['OPS_CA']}"

    flush_interval 5s
    max_retry_wait 300
    disable_retry_limit
  </match>
  <match **>
     @type elasticsearch_dynamic
     host "#{ENV['ES_HOST']}"
     port "#{ENV['ES_PORT']}"
     scheme https
     index_name ${record['kubernetes_namespace_name']}.${Time.at(time).getutc.strftime(@logstash_dateformat)}
     user fluentd
     password changeme

     client_key "#{ENV['ES_CLIENT_KEY']}"
     client_cert "#{ENV['ES_CLIENT_CERT']}"
     ca_file "#{ENV['ES_CA']}"

     flush_interval 5s
     max_retry_wait 300
     disable_retry_limit
  </match>
</label>

<label @SYSTEM>
  <filter **>
    type record_transformer
    enable_ruby
    <record>
      # if host == 'localhost' do a fqdn lookup
      ## we pull the hostname from the host's /etc/hostname mounted at /etc/docker-hostname to correctly identify the hostname
      hostname ${host.eql?('localhost') ? (begin; File.open('/etc/docker-hostname') { |f| f.readline }.rstrip; rescue; host; end) : host}

      # tag_parts = ["system","var","log","messages-20150710"]
      ## we want to correct the time here in cases where we are reading logs from a prior year.  By default Ruby will assume the current date for
      ## parsing, and since syslog messages do not include the year, it will populate the year with the current year which may resolve to a future date
      ## we attempt to use the date format used for rolled over logs 'YYYYMMDD' if possible to get the correct* year, otherwise we subtract the year by 1
      ## *we cannot know if the year is always correct e.g. logs from 20121230 may be in the file 20130101 which would resolve to still be in the past -- 20131230
      time ${ Time.at(time) > Time.now ? (temp_time = Time.parse(Time.at(time).to_s.gsub(Time.at(time).year.to_s, (tag_parts[3].nil? ? Time.at(time).year.to_s : tag_parts[3][9,4]) )).to_datetime.to_s; Time.parse(temp_time) > Time.now ? Time.parse(temp_time.gsub(Time.parse(temp_time).year.to_s, (Time.parse(temp_time).year - 1).to_s )).to_datetime.to_s : Time.parse(temp_time).to_datetime.to_s ) : Time.at(time).to_datetime.to_s }
      version 1.0.6
    </record>
    remove_keys host
  </filter>
  <match **>
    @type elasticsearch_dynamic
    host "#{ENV['OPS_HOST']}"
    port "#{ENV['OPS_PORT']}"
    scheme https
    index_name .operations.${Time.parse(record['time']).getutc.strftime(@logstash_dateformat)}
    user fluentd
    password changeme

    client_key "#{ENV['OPS_CLIENT_KEY']}"
    client_cert "#{ENV['OPS_CLIENT_CERT']}"
    ca_file "#{ENV['OPS_CA']}"

    flush_interval 5s
    max_retry_wait 300
    disable_retry_limit
  </match>
</label>
